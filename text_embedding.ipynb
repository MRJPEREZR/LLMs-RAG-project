{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae8d93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2e9127",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -r '/content/drive/MyDrive/LLMs_Project/requirements.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa77ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#docs_path = '/content/drive/MyDrive/LLMs_Project/docs/'\n",
    "docs_path = '/docs/'\n",
    "#texts_path = '/content/drive/MyDrive/LLMs_Project/texts_extracted/'\n",
    "texts_path = '/texts_extracted/'\n",
    "requirements_path = '/content/drive/MyDrive/LLMs_Project/requirements.txt'\n",
    "print(\"Docs to be processed\")\n",
    "file_list = os.listdir(docs_path)\n",
    "document_count = len(file_list)\n",
    "print(file_list)\n",
    "print(f\"Total documents found: {document_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6203d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymupdf\n",
    "import re\n",
    "import spacy\n",
    "from glob import glob\n",
    "from langchain_text_splitters import SpacyTextSplitter\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    # Remove non-printable control characters\n",
    "    text = re.sub(r'[\\x00-\\x1F\\x7F]', '', text)\n",
    "    \n",
    "    # Normalize whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def extract_text(file_path: str, destination_folder: str) -> str:\n",
    "    text_lines = []\n",
    "    print(f\"Processing file: {file_path}\")\n",
    "    doc = pymupdf.open(file_path)\n",
    "    for page in doc:\n",
    "      text = page.get_text()\n",
    "      text_lines.extend(text.splitlines())\n",
    "      text_lines = [clean_text(line) for line in text_lines]\n",
    "    with open(destination_folder + file_path.split(\"/\")[-1].split(\".\")[0] + \".txt\", \"w\") as file:\n",
    "        file.write(\"\\n\".join(text_lines))\n",
    "        print(f\"Extracted text saved in: {destination_folder + file_path.split(\"/\")[-1].split(\".\")[0] + '.txt'}\")\n",
    "\n",
    "\n",
    "def chunk_text(text: str, chunk_size=500, chunk_overlap=100):\n",
    "    text_splitter = SpacyTextSplitter(\n",
    "        pipeline=\"en_core_web_sm\",  # Uses spaCy for sentence splitting\n",
    "        chunk_size=chunk_size,      # Then groups sentences into chunks of this size\n",
    "        chunk_overlap=chunk_overlap # Adds overlap between chunks\n",
    "    )\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    print(f\"Number of chunks in: {len(chunks)}\")\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9cbe46",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_path in glob(docs_path + \"*.pdf\", recursive=True):\n",
    "    extract_text(file_path, texts_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57be0dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = []\n",
    "for file_path in glob(texts_path + \"*.txt\", recursive=True):\n",
    "    with open(file_path, \"r\") as file:\n",
    "        text = file.read()\n",
    "    content = chunk_text(text)\n",
    "    metadata = {\"source\": file_path.split(\"/\")[-1].split(\".\")[0]}\n",
    "    for chunk in content:\n",
    "        chunks.append({\"chunk\": chunk, \"metadata\": metadata})\n",
    "print(f\"Total number of chunks created: {len(chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d47f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer(\"Qwen/Qwen3-Embedding-0.6B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcab1bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [c[\"chunk\"] for c in chunks]\n",
    "doc_1 = texts[0:333]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05974b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = model.encode(texts, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4101355f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(embeddings))\n",
    "print(len(embeddings[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09320165",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, emb in enumerate(embeddings):\n",
    "    chunks[i][\"embedding\"] = emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05504fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"/content/drive/MyDrive/LLMs_Project/embeddings.json\", \"w\") as f:\n",
    "    json.dump(chunks, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cccd7783",
   "metadata": {},
   "source": [
    "# Milvus\n",
    "https://milvus.io/docs/full_text_search_with_milvus.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108d7801",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import (\n",
    "    MilvusClient,\n",
    "    DataType,\n",
    "    Function,\n",
    "    FunctionType,\n",
    "    AnnSearchRequest,\n",
    "    RRFRanker,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ad355d",
   "metadata": {},
   "outputs": [],
   "source": [
    "uri = \"http://localhost:19530\"\n",
    "collection_name = \"football_docs\"\n",
    "client = MilvusClient(uri=uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7000532",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer_params = {\"tokenizer\": \"standard\", \"filter\": [\"lowercase\"]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0955f7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = MilvusClient.create_schema()\n",
    "schema.add_field(\n",
    "    field_name=\"id\",\n",
    "    datatype=DataType.VARCHAR,\n",
    "    is_primary=True,\n",
    "    auto_id=True,\n",
    "    max_length=100,\n",
    ")\n",
    "schema.add_field(\n",
    "    field_name=\"content\",\n",
    "    datatype=DataType.VARCHAR,\n",
    "    max_length=65535,\n",
    "    analyzer_params=analyzer_params,\n",
    "    enable_match=True,  # Enable text matching\n",
    "    enable_analyzer=True,  # Enable text analysis\n",
    ")\n",
    "schema.add_field(field_name=\"sparse_vector\", datatype=DataType.SPARSE_FLOAT_VECTOR)\n",
    "schema.add_field(\n",
    "    field_name=\"dense_vector\",\n",
    "    datatype=DataType.FLOAT_VECTOR,\n",
    "    dim=1024,  # Dimension for Qwen3-Embedding-0.6B\n",
    ")\n",
    "schema.add_field(field_name=\"metadata\", datatype=DataType.JSON)\n",
    "\n",
    "bm25_function = Function(\n",
    "    name=\"bm25\",\n",
    "    function_type=FunctionType.BM25,\n",
    "    input_field_names=[\"content\"],\n",
    "    output_field_names=\"sparse_vector\",\n",
    ")\n",
    "\n",
    "schema.add_function(bm25_function)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5471ac57",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_params = MilvusClient.prepare_index_params()\n",
    "index_params.add_index(\n",
    "    field_name=\"sparse_vector\",\n",
    "    index_type=\"SPARSE_INVERTED_INDEX\",\n",
    "    metric_type=\"BM25\",\n",
    ")\n",
    "index_params.add_index(field_name=\"dense_vector\", index_type=\"FLAT\", metric_type=\"IP\")\n",
    "\n",
    "if client.has_collection(collection_name):\n",
    "    client.drop_collection(collection_name)\n",
    "client.create_collection(\n",
    "    collection_name=collection_name,\n",
    "    schema=schema,\n",
    "    index_params=index_params,\n",
    ")\n",
    "print(f\"Collection '{collection_name}' created successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f4d12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, doc in enumerate(documents):\n",
    "    entities.append(\n",
    "        {\n",
    "            \"content\": doc[\"content\"],\n",
    "            \"dense_vector\": embeddings[i],\n",
    "            \"metadata\": doc.get(\"metadata\", {}),\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Insert data\n",
    "client.insert(collection_name, entities)\n",
    "print(f\"Inserted {len(entities)} documents\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75907f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"what is hybrid search\"\n",
    "\n",
    "query_embedding = get_embeddings([query])[0]\n",
    "\n",
    "sparse_search_params = {\"metric_type\": \"BM25\"}\n",
    "sparse_request = AnnSearchRequest(\n",
    "    [query], \"sparse_vector\", sparse_search_params, limit=5\n",
    ")\n",
    "\n",
    "dense_search_params = {\"metric_type\": \"IP\"}\n",
    "dense_request = AnnSearchRequest(\n",
    "    [query_embedding], \"dense_vector\", dense_search_params, limit=5\n",
    ")\n",
    "\n",
    "results = client.hybrid_search(\n",
    "    collection_name,\n",
    "    [sparse_request, dense_request],\n",
    "    ranker=RRFRanker(),  # Reciprocal Rank Fusion for combining results\n",
    "    limit=5,\n",
    "    output_fields=[\"content\", \"metadata\"],\n",
    ")\n",
    "hybrid_results = results[0]\n",
    "\n",
    "print(\"\\nHybrid Search (Combined):\")\n",
    "for i, result in enumerate(hybrid_results):\n",
    "    print(\n",
    "        f\"{i+1}. Score: {result['distance']:.4f}, Content: {result['entity']['content']}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5231b6",
   "metadata": {},
   "source": [
    "# Answer generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c2b99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\\n\\n\".join([doc[\"entity\"][\"content\"] for doc in hybrid_results])\n",
    "\n",
    "prompt = f\"\"\"Answer the following question based on the provided context. \n",
    "If the context doesn't contain relevant information, just say \"I don't have enough information to answer this question.\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "response = openai_client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful assistant that answers questions based on the provided context.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
