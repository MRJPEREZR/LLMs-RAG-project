{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bae8d93f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f2e9127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymupdf (from -r /content/drive/MyDrive/LLMs_Project/requirements.txt (line 1))\n",
      "  Downloading pymupdf-1.27.1-cp310-abi3-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (from -r /content/drive/MyDrive/LLMs_Project/requirements.txt (line 2)) (3.8.11)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from -r /content/drive/MyDrive/LLMs_Project/requirements.txt (line 3)) (2.10.0+cpu)\n",
      "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (from -r /content/drive/MyDrive/LLMs_Project/requirements.txt (line 4)) (1.2.10)\n",
      "Requirement already satisfied: langchain-core in /usr/local/lib/python3.12/dist-packages (from -r /content/drive/MyDrive/LLMs_Project/requirements.txt (line 5)) (1.2.13)\n",
      "Collecting langchain-text-splitters (from -r /content/drive/MyDrive/LLMs_Project/requirements.txt (line 6))\n",
      "  Downloading langchain_text_splitters-1.1.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy->-r /content/drive/MyDrive/LLMs_Project/requirements.txt (line 2)) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy->-r /content/drive/MyDrive/LLMs_Project/requirements.txt (line 2)) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy->-r /content/drive/MyDrive/LLMs_Project/requirements.txt (line 2)) (1.0.15)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy->-r /content/drive/MyDrive/LLMs_Project/requirements.txt (line 2)) (2.0.13)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy->-r /content/drive/MyDrive/LLMs_Project/requirements.txt (line 2)) (3.0.12)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy->-r /content/drive/MyDrive/LLMs_Project/requirements.txt (line 2)) (8.3.10)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy->-r /content/drive/MyDrive/LLMs_Project/requirements.txt (line 2)) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy->-r /content/drive/MyDrive/LLMs_Project/requirements.txt (line 2)) (2.5.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy->-r /content/drive/MyDrive/LLMs_Project/requirements.txt (line 2)) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from spacy->-r /content/drive/MyDrive/LLMs_Project/requirements.txt (line 2)) (0.4.3)\n",
      "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy->-r /content/drive/MyDrive/LLMs_Project/requirements.txt (line 2)) (0.24.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.12/dist-packages (from spacy->-r /content/drive/MyDrive/LLMs_Project/requirements.txt (line 2)) (4.67.3)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.12/dist-packages (from spacy->-r /content/drive/MyDrive/LLMs_Project/requirements.txt (line 2)) (2.0.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy->-r /content/drive/MyDrive/LLMs_Project/requirements.txt (line 2)) (2.32.4)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy->-r /content/drive/MyDrive/LLMs_Project/requirements.txt (line 2)) (2.12.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy->-r /content/drive/MyDrive/LLMs_Project/requirements.txt (line 2)) (3.1.6)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy->-r /content/drive/MyDrive/LLMs_Project/requirements.txt (line 2)) (75.2.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy->-r /content/drive/MyDrive/LLMs_Project/requirements.txt (line 2)) (26.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->-r /content/drive/MyDrive/LLMs_Project/requirements.txt (line 3)) (3.24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->-r /content/drive/MyDrive/LLMs_Project/requirements.txt (line 3)) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->-r /content/drive/MyDrive/LLMs_Project/requirements.txt (line 3)) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch->-r /content/drive/MyDrive/LLMs_Project/requirements.txt (line 3)) (3.6.1)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch->-r /content/drive/MyDrive/LLMs_Project/requirements.txt (line 3)) (2025.3.0)\n",
      "Requirement already satisfied: langgraph<1.1.0,>=1.0.8 in /usr/local/lib/python3.12/dist-packages (from langchain->-r /content/drive/MyDrive/LLMs_Project/requirements.txt (line 4)) (1.0.8)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core->-r /content/drive/MyDrive/LLMs_Project/requirements.txt (line 5)) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core->-r /content/drive/MyDrive/LLMs_Project/requirements.txt (line 5)) (0.7.3)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core->-r /content/drive/MyDrive/LLMs_Project/requirements.txt (line 5)) (6.0.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core->-r /content/drive/MyDrive/LLMs_Project/requirements.txt (line 5)) (9.1.4)\n",
      "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core->-r /content/drive/MyDrive/LLMs_Project/requirements.txt (line 5)) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core->-r /content/drive/MyDrive/LLMs_Project/requirements.txt (line 5)) (3.0.0)\n",
      "Requirement already satisfied: langgraph-checkpoint<5.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.8->langchain->-r /content/drive/MyDrive/LLMs_Project/requirements.txt (line 4)) (4.0.0)\n",
      "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.7 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.8->langchain->-r /content/drive/MyDrive/LLMs_Project/requirements.txt (line 4)) (1.0.7)\n",
      "Requirement already satisfied: langgraph-sdk<0.4.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.8->langchain->-r /content/drive/MyDrive/LLMs_Project/requirements.txt (line 4)) (0.3.6)\n",
      "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.8->langchain->-r /content/drive/MyDrive/LLMs_Project/requirements.txt (line 4)) (3.6.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core->-r /content/drive/MyDrive/LLMs_Project/requirements.txt (line 5)) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core->-r /content/drive/MyDrive/LLMs_Project/requirements.txt (line 5)) (3.11.7)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core->-r /content/drive/MyDrive/LLMs_Project/requirements.txt (line 5)) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core->-r /content/drive/MyDrive/LLMs_Project/requirements.txt (line 5)) (0.25.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->-r /content/drive/MyDrive/LLMs_Project/requirements.txt (line 2)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->-r /content/drive/MyDrive/LLMs_Project/requirements.txt (line 2)) (2.41.4)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->-r /content/drive/MyDrive/LLMs_Project/requirements.txt (line 2)) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy->-r /content/drive/MyDrive/LLMs_Project/requirements.txt (line 2)) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy->-r /content/drive/MyDrive/LLMs_Project/requirements.txt (line 2)) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy->-r /content/drive/MyDrive/LLMs_Project/requirements.txt (line 2)) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy->-r /content/drive/MyDrive/LLMs_Project/requirements.txt (line 2)) (2026.1.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->-r /content/drive/MyDrive/LLMs_Project/requirements.txt (line 3)) (1.3.0)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy->-r /content/drive/MyDrive/LLMs_Project/requirements.txt (line 2)) (1.3.3)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy->-r /content/drive/MyDrive/LLMs_Project/requirements.txt (line 2)) (0.1.5)\n",
      "Requirement already satisfied: typer>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim<1.0.0,>=0.3.0->spacy->-r /content/drive/MyDrive/LLMs_Project/requirements.txt (line 2)) (0.24.0)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy->-r /content/drive/MyDrive/LLMs_Project/requirements.txt (line 2)) (0.23.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy->-r /content/drive/MyDrive/LLMs_Project/requirements.txt (line 2)) (7.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy->-r /content/drive/MyDrive/LLMs_Project/requirements.txt (line 2)) (3.0.3)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core->-r /content/drive/MyDrive/LLMs_Project/requirements.txt (line 5)) (4.12.1)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core->-r /content/drive/MyDrive/LLMs_Project/requirements.txt (line 5)) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core->-r /content/drive/MyDrive/LLMs_Project/requirements.txt (line 5)) (0.16.0)\n",
      "Requirement already satisfied: ormsgpack>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<5.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.8->langchain->-r /content/drive/MyDrive/LLMs_Project/requirements.txt (line 4)) (1.12.2)\n",
      "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy->-r /content/drive/MyDrive/LLMs_Project/requirements.txt (line 2)) (2.1.1)\n",
      "Requirement already satisfied: click>=8.2.1 in /usr/local/lib/python3.12/dist-packages (from typer>=0.24.0->typer-slim<1.0.0,>=0.3.0->spacy->-r /content/drive/MyDrive/LLMs_Project/requirements.txt (line 2)) (8.3.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.24.0->typer-slim<1.0.0,>=0.3.0->spacy->-r /content/drive/MyDrive/LLMs_Project/requirements.txt (line 2)) (1.5.4)\n",
      "Requirement already satisfied: rich>=12.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.24.0->typer-slim<1.0.0,>=0.3.0->spacy->-r /content/drive/MyDrive/LLMs_Project/requirements.txt (line 2)) (13.9.4)\n",
      "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from typer>=0.24.0->typer-slim<1.0.0,>=0.3.0->spacy->-r /content/drive/MyDrive/LLMs_Project/requirements.txt (line 2)) (0.0.4)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=12.3.0->typer>=0.24.0->typer-slim<1.0.0,>=0.3.0->spacy->-r /content/drive/MyDrive/LLMs_Project/requirements.txt (line 2)) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=12.3.0->typer>=0.24.0->typer-slim<1.0.0,>=0.3.0->spacy->-r /content/drive/MyDrive/LLMs_Project/requirements.txt (line 2)) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=12.3.0->typer>=0.24.0->typer-slim<1.0.0,>=0.3.0->spacy->-r /content/drive/MyDrive/LLMs_Project/requirements.txt (line 2)) (0.1.2)\n",
      "Downloading pymupdf-1.27.1-cp310-abi3-manylinux_2_28_x86_64.whl (24.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.9/24.9 MB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading langchain_text_splitters-1.1.1-py3-none-any.whl (35 kB)\n",
      "Installing collected packages: pymupdf, langchain-text-splitters\n",
      "Successfully installed langchain-text-splitters-1.1.1 pymupdf-1.27.1\n"
     ]
    }
   ],
   "source": [
    "pip install -r '/content/drive/MyDrive/LLMs_Project/requirements.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa77ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docs to be processed\n",
      "Total documents found: 4\n",
      "['Laws of the Game 2025_26_single pages.pdf', 'FIFA Disciplinary Code_September 2025 edition_EN.pdf', 'FIFA Equipment Regulations_2025_EN.pdf', 'FWC26_Competition Regulations_EN.pdf']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "docs_path = '/content/drive/MyDrive/LLMs_Project/docs/'\n",
    "texts_path = '/content/drive/MyDrive/LLMs_Project/texts_extracted/'\n",
    "sentences_path = '/content/drive/MyDrive/LLMs_Project/sentences_extracted/'\n",
    "chunks_path = '/content/drive/MyDrive/LLMs_Project/chunks_extracted/'\n",
    "requirements_path = '/content/drive/MyDrive/LLMs_Project/requirements.txt'\n",
    "print(\"Docs to be processed\")\n",
    "file_list = os.listdir(docs_path)\n",
    "document_count = len(file_list)\n",
    "print(file_list)\n",
    "print(f\"Total documents found: {document_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fe6203d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymupdf\n",
    "import re\n",
    "import spacy\n",
    "from glob import glob\n",
    "from langchain_text_splitters import SpacyTextSplitter\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    # Remove non-printable control characters\n",
    "    text = re.sub(r'[\\x00-\\x1F\\x7F]', '', text)\n",
    "    \n",
    "    # Normalize whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def extract_text(file_path: str, destination_folder: str) -> str:\n",
    "    text_lines = []\n",
    "    print(f\"Processing file: {file_path}\")\n",
    "    doc = pymupdf.open(file_path)\n",
    "    for page in doc:\n",
    "      text = page.get_text()\n",
    "      text_lines.extend(text.splitlines())\n",
    "      text_lines = [clean_text(line) for line in text_lines]\n",
    "    with open(destination_folder + file_path.split(\"/\")[-1].split(\".\")[0] + \".txt\", \"w\") as file:\n",
    "        file.write(\"\\n\".join(text_lines))\n",
    "        print(f\"Extracted text saved in: {destination_folder + file_path.split(\"/\")[-1].split(\".\")[0] + '.txt'}\")\n",
    "\n",
    "\n",
    "def chunk_text(text: str, chunk_size=500, chunk_overlap=100):\n",
    "    text_splitter = SpacyTextSplitter(\n",
    "        pipeline=\"en_core_web_sm\",  # Uses spaCy for sentence splitting\n",
    "        chunk_size=chunk_size,              # Then groups sentences into chunks of this size\n",
    "        chunk_overlap=chunk_overlap            # Adds overlap between chunks\n",
    "    )\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    print(f\"Number of chunks in: {len(chunks)}\")\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3e9cbe46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: /content/drive/MyDrive/LLMs_Project/docs/Laws of the Game 2025_26_single pages.pdf\n",
      "Extracted text saved in: /content/drive/MyDrive/LLMs_Project/texts_extracted/Laws of the Game 2025_26_single pages.txt\n",
      "Processing file: /content/drive/MyDrive/LLMs_Project/docs/FIFA Disciplinary Code_September 2025 edition_EN.pdf\n",
      "Extracted text saved in: /content/drive/MyDrive/LLMs_Project/texts_extracted/FIFA Disciplinary Code_September 2025 edition_EN.txt\n",
      "Processing file: /content/drive/MyDrive/LLMs_Project/docs/FIFA Equipment Regulations_2025_EN.pdf\n",
      "Extracted text saved in: /content/drive/MyDrive/LLMs_Project/texts_extracted/FIFA Equipment Regulations_2025_EN.txt\n",
      "Processing file: /content/drive/MyDrive/LLMs_Project/docs/FWC26_Competition Regulations_EN.pdf\n",
      "Extracted text saved in: /content/drive/MyDrive/LLMs_Project/texts_extracted/FWC26_Competition Regulations_EN.txt\n"
     ]
    }
   ],
   "source": [
    "for file_path in glob(docs_path + \"*.pdf\", recursive=True):\n",
    "    extract_text(file_path, texts_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "57be0dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:langchain_text_splitters.base:Created a chunk of size 696, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 538, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 1577, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 512, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 524, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 783, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 856, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 711, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 1266, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 971, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 599, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 1156, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 528, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 723, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 652, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 502, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 532, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 743, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 854, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 557, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 533, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 609, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 800, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 912, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 1247, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 647, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 1087, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 1275, which is longer than the specified 500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks in: 234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:langchain_text_splitters.base:Created a chunk of size 883, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 1024, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 831, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 663, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 805, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 707, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 658, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 917, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 506, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 646, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 559, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 1069, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 736, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 1184, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 544, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 581, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 11865, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 1162, which is longer than the specified 500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks in: 284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:langchain_text_splitters.base:Created a chunk of size 575, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 502, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 517, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 672, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 825, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 811, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 799, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 850, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 527, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 713, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 542, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 523, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 551, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 1074, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 1026, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 553, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 501, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 1139, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 637, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 559, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 560, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 827, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 795, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 1155, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 1082, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 989, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 693, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 669, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 604, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 581, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 620, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 576, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 510, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 525, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 894, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 620, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 536, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 670, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 565, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 642, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 580, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 677, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 810, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 807, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 607, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 647, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 572, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 1260, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 655, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 523, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 506, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 507, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 605, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 643, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 526, which is longer than the specified 500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks in: 334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:langchain_text_splitters.base:Created a chunk of size 758, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 649, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 549, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 517, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 602, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 1704, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 1785, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 1040, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 562, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 958, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 504, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 575, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 739, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 1062, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 559, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 629, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 963, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 694, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 525, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 786, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 1126, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 883, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 602, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 1072, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 1934, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 566, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 720, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 512, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 767, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 666, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 635, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 660, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 953, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 1601, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 3134, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 944, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 647, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 2237, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 589, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 555, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 555, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 653, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 2325, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 828, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 1039, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 690, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 555, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 655, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 741, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 986, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 730, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 1186, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 809, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 643, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 1667, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 506, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 685, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 811, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 537, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 818, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 560, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 532, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 536, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 801, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 532, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 571, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 518, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 537, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 561, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 621, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 568, which is longer than the specified 500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks in: 466\n"
     ]
    }
   ],
   "source": [
    "\n",
    "chunks = []\n",
    "for file_path in glob(texts_path + \"*.txt\", recursive=True):\n",
    "    with open(file_path, \"r\") as file:\n",
    "        text = file.read()\n",
    "    content = chunk_text(text)\n",
    "    metadata = {\"source\": file_path.split(\"/\")[-1].split(\".\")[0]}\n",
    "    for chunk in content:\n",
    "        chunks.append({\"chunk\": chunk, \"metadata\": metadata})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5dbf2563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of chunks created: 1318\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total number of chunks created: {len(chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0882604b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requires transformers>=4.51.0\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import Tensor\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44e1cb29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cac0b3fe5e3b44bd823d2e0fcf1e57b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/310 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Qwen3Model(\n",
       "  (embed_tokens): Embedding(151669, 1024)\n",
       "  (layers): ModuleList(\n",
       "    (0-27): 28 x Qwen3DecoderLayer(\n",
       "      (self_attn): Qwen3Attention(\n",
       "        (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
       "        (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "        (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "        (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "      )\n",
       "      (mlp): Qwen3MLP(\n",
       "        (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "        (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "        (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "        (act_fn): SiLUActivation()\n",
       "      )\n",
       "      (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
       "      (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
       "    )\n",
       "  )\n",
       "  (norm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
       "  (rotary_emb): Qwen3RotaryEmbedding()\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"Qwen/Qwen3-Embedding-0.6B\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    padding_side=\"left\"\n",
    ")\n",
    "\n",
    "\n",
    "model = AutoModel.from_pretrained(model_name).to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0845571",
   "metadata": {},
   "outputs": [],
   "source": [
    "def last_token_pool(last_hidden_states: Tensor,\n",
    "                    attention_mask: Tensor) -> Tensor:\n",
    "    left_padding = (attention_mask[:, -1].sum() == attention_mask.shape[0])\n",
    "    if left_padding:\n",
    "        return last_hidden_states[:, -1]\n",
    "    else:\n",
    "        sequence_lengths = attention_mask.sum(dim=1) - 1\n",
    "        batch_size = last_hidden_states.shape[0]\n",
    "        return last_hidden_states[\n",
    "            torch.arange(batch_size, device=last_hidden_states.device),\n",
    "            sequence_lengths\n",
    "        ]\n",
    "\n",
    "def get_embeddings(texts: List[str],\n",
    "                   task_description: str = None,\n",
    "                   max_length: int = 8192) -> List[List[float]]:\n",
    "\n",
    "    if not texts:\n",
    "        return []\n",
    "\n",
    "    batch_dict = tokenizer(\n",
    "        texts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    batch_dict = {k: v.to(device) for k, v in batch_dict.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch_dict)\n",
    "\n",
    "    embeddings = last_token_pool(\n",
    "        outputs.last_hidden_state,\n",
    "        batch_dict[\"attention_mask\"]\n",
    "    )\n",
    "\n",
    "    # Normalize (for cosine similarity search)\n",
    "    embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "\n",
    "    return embeddings.cpu().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "dcab1bb1",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "texts = [c[\"chunk\"] for c in chunks]\n",
    "embeddings = get_embeddings(texts)\n",
    "\n",
    "for i, emb in enumerate(embeddings):\n",
    "    chunks[i][\"embedding\"] = emb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cccd7783",
   "metadata": {},
   "source": [
    "# Milvus\n",
    "https://milvus.io/docs/full_text_search_with_milvus.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108d7801",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import (\n",
    "    MilvusClient,\n",
    "    DataType,\n",
    "    Function,\n",
    "    FunctionType,\n",
    "    AnnSearchRequest,\n",
    "    RRFRanker,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ad355d",
   "metadata": {},
   "outputs": [],
   "source": [
    "uri = \"http://localhost:19530\"\n",
    "collection_name = \"full_text_demo\"\n",
    "client = MilvusClient(uri=uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7000532",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer_params = {\"tokenizer\": \"standard\", \"filter\": [\"lowercase\"]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0955f7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = MilvusClient.create_schema()\n",
    "schema.add_field(\n",
    "    field_name=\"id\",\n",
    "    datatype=DataType.VARCHAR,\n",
    "    is_primary=True,\n",
    "    auto_id=True,\n",
    "    max_length=100,\n",
    ")\n",
    "schema.add_field(\n",
    "    field_name=\"content\",\n",
    "    datatype=DataType.VARCHAR,\n",
    "    max_length=65535,\n",
    "    analyzer_params=analyzer_params,\n",
    "    enable_match=True,  # Enable text matching\n",
    "    enable_analyzer=True,  # Enable text analysis\n",
    ")\n",
    "schema.add_field(field_name=\"sparse_vector\", datatype=DataType.SPARSE_FLOAT_VECTOR)\n",
    "schema.add_field(\n",
    "    field_name=\"dense_vector\",\n",
    "    datatype=DataType.FLOAT_VECTOR,\n",
    "    dim=1536,  # Dimension for text-embedding-3-small\n",
    ")\n",
    "schema.add_field(field_name=\"metadata\", datatype=DataType.JSON)\n",
    "\n",
    "bm25_function = Function(\n",
    "    name=\"bm25\",\n",
    "    function_type=FunctionType.BM25,\n",
    "    input_field_names=[\"content\"],\n",
    "    output_field_names=\"sparse_vector\",\n",
    ")\n",
    "\n",
    "schema.add_function(bm25_function)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5471ac57",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_params = MilvusClient.prepare_index_params()\n",
    "index_params.add_index(\n",
    "    field_name=\"sparse_vector\",\n",
    "    index_type=\"SPARSE_INVERTED_INDEX\",\n",
    "    metric_type=\"BM25\",\n",
    ")\n",
    "index_params.add_index(field_name=\"dense_vector\", index_type=\"FLAT\", metric_type=\"IP\")\n",
    "\n",
    "if client.has_collection(collection_name):\n",
    "    client.drop_collection(collection_name)\n",
    "client.create_collection(\n",
    "    collection_name=collection_name,\n",
    "    schema=schema,\n",
    "    index_params=index_params,\n",
    ")\n",
    "print(f\"Collection '{collection_name}' created successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f4d12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, doc in enumerate(documents):\n",
    "    entities.append(\n",
    "        {\n",
    "            \"content\": doc[\"content\"],\n",
    "            \"dense_vector\": embeddings[i],\n",
    "            \"metadata\": doc.get(\"metadata\", {}),\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Insert data\n",
    "client.insert(collection_name, entities)\n",
    "print(f\"Inserted {len(entities)} documents\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
